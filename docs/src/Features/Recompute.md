# Recompute

## Why

Some analysis files that are generated by Spyglass are very unlikely to be
reaccessed. Those generated by `SpikeSortingRecording` tables were identified as
taking up tens of terabytes of space, while very seldom accessed after their
first generation. By finding a way to recompute these files on demand, we can
save significant server space at the cost of an unlikely 10m of recompute time
per file.

Spyglass 0.5.5 introduces the opportunity to delete and recompute both newly
generated files after this release, and old files that were generated before
this release.

## How

`SpikeSortingRecording` has a new `_make_file` method that will be called in the
event a file is accessed but not found. This method will generate the file and
compare it's hash to the hash of the file that was expected. If the hashes
match, the file will be saved and returned. If the hashes do not match, the file
will be deleted and an error raised. For steps to avoid such errors, see the
steps below.

### New files

Newly generated files will automatically record information about their
dependencies and the code that generated them in `RecomputeSelection` tables. To
see the dependencies of a file, you can access `RecordingRecomputeSelection`

```python
from spyglass.spikesorting.v1 import recompute as v1_recompute

v1_recompute.RecordingRecomputeSelection()
```

### Old files

To ensure the replicability of old files prior to deletion, we'll need to...

1. Update the tables for new fields.
2. Attempt file recompute, and record dependency info for successful attempts.

<!-- TODO: add code snippet. 2 or 3 tables?? -->

```python
from spyglass.spikesorting.v0 import spikesorting_recording as v0_recording
from spyglass.spikesorting.v1 import recording as v1_recording

# Alter tables to include new fields, updating values
v0_recording.SpikeSortingRecording().alter()
v0_recording.SpikeSortingRecording().update_ids()
v1_recording.SpikeSortingRecording().alter()
v1_recording.SpikeSortingRecording().update_ids()
```
