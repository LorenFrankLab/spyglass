# Recompute

## Where

- `spyglass.spikesorting.v0.spikesorting_recording.SpikeSortingRecording`
- `spyglass.spikesorting.v1.recording.SpikeSortingRecording`

## Why

Some analysis files that are generated by Spyglass are very unlikely to be
reaccessed. Those generated by `SpikeSortingRecording` tables were identified as
taking up tens of terabytes of space, while very seldom accessed after their
first generation. By finding a way to recompute these files on demand, we can
save significant server space at the cost of an unlikely 10m of recompute time
per file.

Spyglass 0.5.5 introduces the opportunity to delete and recompute both newly
generated files after this release, and old files that were generated before
this release.

## How

Common methods like `fetch_nwb` will now check for the existence of a relevant
file(s) and, if missing, will attempt to recompute the file via the `_make_file`
method. For current and future cases, this is a table method that will generate
the requisite files, check their hash, and save them if the hash matches the
expected hash. If the hash does not match, the file(s) will be deleted and an
error raised.

To generate and maintain records of these hashes and dependencies required,
we've added downstream recompute tables for each places where this feature is
implemented.

### New files

Newly generated files will automatically record information about their
dependencies and the code that generated them in `RecomputeSelection` tables. To
see the dependencies of a file, you can access `RecordingRecomputeSelection`.
The only requirements for setting up this feature are modifying the existing
table structure to include the new fields.

=== "v0"
    ```python
    from spyglass.spikesorting.v0 import spikesorting_recording as v0_recording

    # Alter tables to include new fields, updating values
    v0_recording.SpikeSortingRecording().alter()
    v0_recording.SpikeSortingRecording().update_ids()
    ```

=== "v1"
    ```python
    from spyglass.spikesorting.v1 import recording as v1_recording

    # Alter tables to include new fields, updating values
    v1_recording.SpikeSortingRecording().alter()
    v1_recording.SpikeSortingRecording().update_ids()
    ```

### Old files

Retroactively demonstraing that files can be recomputed requires some additional
work in testing various dependencies and recording the results. To ensure the
replicability of old files prior to deletion, we'll need to...

1. Update the tables for new fields, as shown above.
2. Attempt file recompute, and record dependency info for successful attempts.

=== "v0"
    ```python
    from spyglass.spikesorting.v0 import spikesorting_recording as v0_recording
    from spyglass.spikesorting.v0 import spikesorting_recompute as v0_recording

    # Alter tables to include new fields, updating values
    my_keys = (v0_recording.SpikeSortingRecording() & restriction).fetch("KEY")
    v0_recompute.RecordingRecomputeSelection().insert(my_keys)
    v0_recompute.RecordingRecompute().populate()
    ```

=== "v1"
    ```python
    from spyglass.spikesorting.v1 import recording as v1_recording
    from spyglass.spikesorting.v1 import recompute as v1_recompute

    # Alter tables to include new fields, updating values
    my_keys = (v1_recording.SpikeSortingRecording() & restriction).fetch("KEY")
    v1_recompute.RecordingRecomputeVersions().populate(my_keys)
    v1_recompute.RecordingRecomputeSelection().insert(my_keys)
    v1_recompute.RecordingRecompute().populate(my_keys)
    ```

    Optionally, you can set your preferred precision for recompute when inserting
    into `RecordingRecomputeSelection`. The default is 4.

By default, these insert methods will generate an `attempt_id` that serves as a
unique identifier for relevant pip dependencies. You can customize this
identifier by passing an additional `attempt_id` argument to the insert method.

## Folders vs. NWB files

The implementation for tables that generate folders (e.g.,
`v0.SpikeSortingRecording`) differs from those that generate NWB files (e.g.,
`v1.SpikeSortingRecording`). NWB files are record some information about their
dependencies and the code that generated them. We can use this information to
prevent recompute attempts that mismatch. See `RecomputeVersions` tables for
more information.

NWBs also store data objects in a more structured way, which allows us to make
decisions about the degree of precision required for recompute attempts.

## Misc Notes

`match` and `precision` are MySQL keywords, so we use `matched` and `rounding`,
respectively.
